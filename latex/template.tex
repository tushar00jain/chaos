\documentclass[12pt, letterpaper]{article}
\usepackage{blindtext}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{argmin}

\title{Chaotic Neural Networks and Its Applications}
\author{Christian Gould, Tushar Jain}
\date{}

\begin{document}

\maketitle

\section*{Introduction}

The Hopfield neural network [3] is a type of recurrent neural network that serves as an associative memory, which allows its users to retrieve a stored pattern closest to the provided input pattern. The dependence on an input, however, is one of the network's limitations compared to the human brain. The brain is capable of transitioning from one state to another even in the lack of an external stimulus. Unlike the Hopfield neural network, the brain does not get stuck in the state of the last pattern that was recalled. It was conjectured in [4] that the brain is able to do so because it is innately chaotic but transitions to a periodic behaviour when it focuses on stimuli, thereby recalling stored memories. Modifications to the Hopfield Network that make it chaotic and capable of recalling patterns in the same way have been proposed. One such model, M-AdNN is the focus of this review.

\section*{Methodology}

In a Hopfield Network, viewed as associative memmory, the input pattern $y$ is updated using the following rule,

$$
\begin{aligned}
& y_i(t+1) &&= \Theta\big(\sum_{j \neq i} w_{ij}y_j(t) + b_i \big)\\
& \Theta(z) &&= \begin{cases}
  +1, & z > 0.\\
  -1, & z \le 0.
  \end{cases}
\end{aligned}
$$

where $b_i$ denotes bias of the $i$'th neuron.

The Hopfield neural network minimizes the energy function defined as,

$$
E = -\sum_{i,j < i} w_{ij}y_i y_j - \sum_{i} b_i y_i
$$

The weights matrix that minimizes the energy function is,

$$
w_{ij} = \frac{1}{p} \sum_{s=1}^p x_i^s x_j^s
$$

where $x^s$ represents the pattern that is to be stored in the network, with $p$ being the number of patterns to be stored.

\subsection*{Pattern Recognition}

In order to emulate the brain's behaviour of recalling patterns from stimuli, Adachi’s Neural Network (AdNN) [5] was proposed. It was that shown that M-AdNN [1] is a truly chaotic neural network, which exhibits periodicity when the input pattern closely resembles one of the stored states. The M-AdNN prevents being attracted to a local minima of the energy function by remaining in a chaotic state when the input pattern does not resonate with any of the stored patterns.

The M-AdNN updates the input pattern $y$ using the rule,

$$
\begin{aligned}
& y_i(t+1) &&= f(\eta_i(t+1) + \xi_i(t+1))\\
& \eta_i(t+1) &&= k_f \eta_N(t) + \sum_{j = 1}^N w_{ij} y_j(t)\\
& \xi_i(t+1) &&= k_r \xi_N(t) - \alpha y_i(t) + a_i
\end{aligned}
$$

Here, $f$ is the logistic function, $f(u) = \frac{1}{1 + e^\frac{-u}{\varepsilon}}$, where $\varepsilon$ is the parameter that controls the steepness. The logistic function plays the role of squashing the parameters between $0$ and $1$. $k_f$ and $k_r$ are constant parameters, that have the biological significance of decay with respect to feedback inputs and refractoriness. $a_i$ is a constant external input to the $i^{\text{th}}$ neuron, which is taken to be $a_i = y_i$, in order to make network more receptive to the input. It is showed that M-AdNN is chaotic using analysis of the lyapunov spectrum. The M-AdNN has the Jacobian matrix, starting at the initial point $A$, is of the form

$$
\begin{aligned}
& J(A) &&= \begin{pmatrix}
  [J_{ij}^1] & [J_{ij}^2]\\
  [J_{ij}^3] & [J_{ij}^4]
  \end{pmatrix}\\
\end{aligned}
$$

$J_{ij}^1(t) = \frac{\partial \eta_i(t+1)}{\partial \eta_j(t)}, J_{ij}^2(t) = \frac{\partial \eta_i(t+1)}{\partial \xi_j(t)}, J_{ij}^3(t) = \frac{\partial \xi_i(t+1)}{\partial \eta_j(t)}, J_{ij}^4(t) = \frac{\partial \xi_i(t+1)}{\partial \xi_j(t)}$

The lyapunov exponents of the system are given by the natural logarithm of the eigenvalues of the matrix $\Lambda_A := [J(A)J(A)^T]^{\frac{1}{2}}$, which are discovered to be ${\frac{1}{2}}ln(N) + ln(k_f)$ and ${\frac{1}{2}}ln(N) + ln(k_r)$, and the rest of the lyapunov exponents being $-\infty$. Therefore, if either of the finite lyapunov are positive, the system exhibits chaotic behaviour. Although, it appears that $k_f$ and $k_r$ determine the behaviour of the system, it is also discovered that $\alpha$ also plays an important role as a bifurcation parameter [8]. As a system capable of pattern recognition, it can determined weather the M-AdNN has converged to a periodic orbit by studying computing the distance from each of the stored patterns. The periodicity was measured to be the number of steps between each step where the output was observed to be close to the same stored pattern. After a few transient steps, it was observed that the inputs resonated with only one of the stored pattern. Although possible that the output can resonate with another stored pattern, the authors argued that they did not observe such behaviour in the 1000 iterations they ran. If it were the case that there was another attractor, its periodicity would be orders of magnitude higher than relatively lower periodicity observed for the trained pattern initially. In such case, the input pattern is classified as the stored pattern with lower periodicity.


\subsection*{Cryptography}
In [6] A bifurcation analysis is undertaken for \ref{Null Solution Stability} a discrete-time Hopfield neural network of two neurons with two delays, two internal decays and no self-connections, with the product of the interconnection coefficients as the characteristic parameter for the system. In this paper, The stability domain of the null solution is found, the values of the characteristic parameter for which bifurcations occur at the origin, are identified, and the existence of Fold/Cusp, Neimark–Sacker and Flip bifurcations is proved. All these bifurcations are analyzed by applying the center manifold theorem and the normal form theory. It is shown that the dynamics in a neighborhood of the null solution become more and more complex as the characteristic parameter grows in magnitude and passes through the bifurcation values. Under certain conditions, it is proved that if the magnitudes of the interconnection coefficients are large enough, the neural network exhibits Marotto’s chaotic behavior. The Network Discussed in [ 6 7] has the following general form:

\begin{equation}\label{hopfield network}
    \begin{cases}
    x_{n+1} = a_{1}x_{n} + T_{12}g_{2}(y_{n - k_{2}})\\
    y_{n+1} = a_{2}y_{n} + T_{21}g_{1}(x_{n - k_{1}})
    \end{cases} \forall n \geq \text{max}(k1, k2)
\end{equation}


In [7] a Cryptosystem based on the above system is presented and analyzed. This network is utilized to generate binary sequences that are used in permuting each plaintext block, in encrypting it, and in choosing the trajectory of the next iteration. Another novelty of this work is that the trajectory may be switched for every term of the binary sequence, providing a higher degree of randomness. the Authors have based their conclusions on the study of image encryption, for it is known that, due to some inherent features of images, such as bulk data capacity and high correlation among pixels, image encryption is more complex than
the usual text encryption. This paper describes the class of Hopfield neural networks
used for encryption-decryption. Presents the proposed encryption algorithm, and shows
that decryption is completely possible. Then the Authors exemplify this on a particular Hopfield neural network with delays. The security of the system is analyzed, where there is a check on the resistance to statistical and differential attack and Finally the entropy and key are computed for
security. Papers [6 7] together create an end to end analysis and implementation of a discrete chaotic cryptosystem with delays.

In paper [6] the Dynamics of neural networks focuses on three different components: Bifurcation analysis with respect to the characteristic parameter b:

\begin{equation}\label{b defintion}
b = T_{12}g_{2}(0)^{'} \cdot T_{21}g_{1}(0)^{'}
\end{equation}

The set of values of b, for which the null solution is asymptotically stable is identified, then those values of b that produce bifurcations at the origin are found, and then analyzed using both, center manifold theorem, and normal form theory. Finally, the paper shows that if the activation functions satisfy specific conditions, and the magnitudes of the connection coefficients \(T_{12}, T_{21}\), are "large enough", the neural network exhibits chaotic behavior near the origin.

It is not clarified in this paper as to why the product of the interconnection coefficients is the characteristic parameter of the system but there are resources outside this paper that do, which needs to be explored, but for now we just take this as it is. To find the null solution, the system is converted into a standard matrix form without delays in order to compute the Jacobean. Once the Jacobean is computed, the eigenvalues are calculated as the following characteristic equation is formed
\begin{equation}\label{characteristic equation 1}
z^{k_{1} + k_{2}}(z - a_{1})(z - a_{2})  = z^{k}(z - a_{1})(z - a_{2}) - b
\end{equation}
note that the parameter b is concatenated onto the characteristic equation, setting the characteristic equation to zero so as to find the eigenvalues we get:

\begin{equation}\label{characteristic equation 2}
    \text{Char}(\lambda) = P_{a,k}(z) = b
\end{equation}
we are then shown that the null solution of \ref{characteristic equation 2} is stable when the parameters for b are in the range:

\begin{equation}\label{Null Solution Stability}
    b \in D_{S}(a,k) = (b_{a,k}^{1}, b_{a,k}^{0})
\end{equation}

Taking into consideration the interest in finding the chaotic solution to this hopfield network, we move to section 3 where the paper discusses Marottos Chaos:

\begin{theorem}(Marotto). Suppose that the continuously differentiable function \(f:\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\) has a snap-back repeller \(z*\), ie:
\begin{enumerate}
    \item z* is a fixed point of f;
    \item all eigenvalues of \(Df(z*)\) exceed 1 in magnitude;
    \item there exists a point \(z_{0} \neq z*\) in a repelling neighborhood of \(z*\) such that \(z_M = z*\) and \text{det}\((Df(z_{j}))\) for \(1 \leq j \leq M\), where \(z_{j} = f^{j}(z_{0})\) ( for some integer M )\\
\end{enumerate}
then f is chaotic
\end{theorem}

\begin{lemma} if the following is true:
\[
|T_{12}T_{21}| \geq \frac{(1+a_{1})(1 + a_{2})}{|g'_{1}(0)g'_{2}(0)|} + \epsilon = T*
\]
then all the eigenvalues have magnitude greater than 1
\end{lemma}

now that the system is better defined we move to a specific example defined by paper [7]

\[
    \begin{cases}
        x_{n+1} = \frac{1}{4}x_{n} + tanh(y_{n - 2})\\
        y_{n+1} = \frac{3}{4}y_{n} + sin(x_{n - 1})
    \end{cases}
\]

Because we already verified in [6] that this particular system is chaotic, we need only implement the encryption algorithm.

\section*{Conclusion}

An immediate application of the chaotic pattern recognition could be to OCR (Optical Character Recognition) mechanism.

\section*{References}

\fontsize{8}{12}\selectfont

\begin{enumerate}[leftmargin=*]
    \item Calitoiu, D., Oommen, B.J. \& Nussbaum, D. Periodicity and stability issues of a chaotic pattern recognition neural network. Pattern Anal Applic 10, 175–188 (2007). https://doi.org/10.1007/s10044-007-0060-3
    \item Darau, Mirela \& Kaslik, Eva \& Balint, Stefan. (2012). Cryptography using chaotic discrete-time delayed Hopfield neural networks. Mathematics in Engineering, Science and Aerospace MESA. 1.
    \item Hopfield, J.J. Neural networks and physical systems with emergent collective computational abilities.
    Proc. Natl. Acad. Sci. USA 1982. 79, 2554–2558.
    \item Freeman WJ (1992) Tutorial in neurobiology: from single neurons to brain chaos. Int J Bifurcat Chaos 2:451–482
    \item Adachi M, Aihara K (1997) Associative dynamics in a cha- otic neural network. Neural Netw 10:83–98
    \item Adachi M, Aihara K, et al. “Chaotic Dynamics of a Delayed Discrete-Time Hopfield Network of Two Nonidentical Neurons with no self-connections.” Journal of Nonlinear Science, Springer-Verlag, 1 Jan. 1997, link.springer.com/article/10.1007/s00332-007-9015-5.
    \item Darau, Mirela, et al. “Cryptography Using Chaotic Discrete-Time Delayed Hopfield Neural Networks.” Journal | MESA, 25 Feb. 2012, nonlinearstudies.com/index.php/mesa/article/view/703.
    \item Qin, Ke \& Oommen, B.. (2008). Chaotic Pattern Recognition: The Spectrum of Properties of the Adachi Neural Network.
\end{enumerate}

\end{document}
